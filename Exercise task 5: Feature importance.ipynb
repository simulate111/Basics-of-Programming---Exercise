{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7ad86de09ed04a039eca6365e64e466e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_92e4dc7b53794da39021519b75773186",
              "IPY_MODEL_b2ce206f0d9746e491de89fd732f1f75",
              "IPY_MODEL_7d028b8fff074f2aa1b4846b8cf3449d"
            ],
            "layout": "IPY_MODEL_10e6cf26c3744d44900173a92228aaf4"
          }
        },
        "92e4dc7b53794da39021519b75773186": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f7c0905db02458ba16eb5b24fd9e3db",
            "placeholder": "​",
            "style": "IPY_MODEL_9ea0d7f22301421fb9245708a7e5d49b",
            "value": "Map (num_proc=4): 100%"
          }
        },
        "b2ce206f0d9746e491de89fd732f1f75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab2ff9d720a040b58e5a9136695fa20f",
            "max": 25000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3ce8b6ec4d894d03845a0aad3ff12d0b",
            "value": 25000
          }
        },
        "7d028b8fff074f2aa1b4846b8cf3449d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1fe50785bccf46c0ba56b068d1e271b1",
            "placeholder": "​",
            "style": "IPY_MODEL_e25bada82ee84d3db5a58ca7bd2a9e71",
            "value": " 25000/25000 [00:29&lt;00:00, 840.27 examples/s]"
          }
        },
        "10e6cf26c3744d44900173a92228aaf4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f7c0905db02458ba16eb5b24fd9e3db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ea0d7f22301421fb9245708a7e5d49b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ab2ff9d720a040b58e5a9136695fa20f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ce8b6ec4d894d03845a0aad3ff12d0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1fe50785bccf46c0ba56b068d1e271b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e25bada82ee84d3db5a58ca7bd2a9e71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "002744b49222446fbf8026508a6564c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2eb66e05993e4adca1ac779611d36e8c",
              "IPY_MODEL_006d453afb4d4573a0b9dad2491789e8",
              "IPY_MODEL_91c7bbeb27d544559d4e340bce695ab7"
            ],
            "layout": "IPY_MODEL_a341e94e390a4cf28df754922b48fe5f"
          }
        },
        "2eb66e05993e4adca1ac779611d36e8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34e993cb329a47d2bd38317fce26fc85",
            "placeholder": "​",
            "style": "IPY_MODEL_ce8256917bda46b998d9cc1d12d0ab9e",
            "value": "Map (num_proc=4): 100%"
          }
        },
        "006d453afb4d4573a0b9dad2491789e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94c5a05d22db4d489fff96cda7400f9d",
            "max": 25000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fc9c3f1ab61b44a0af226c56d2d23fb0",
            "value": 25000
          }
        },
        "91c7bbeb27d544559d4e340bce695ab7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d17135271bce4cd0ac04cf9d99df4f0d",
            "placeholder": "​",
            "style": "IPY_MODEL_4eea957cdf7c4112b7ad573dc3a0c4d4",
            "value": " 25000/25000 [00:25&lt;00:00, 1244.27 examples/s]"
          }
        },
        "a341e94e390a4cf28df754922b48fe5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34e993cb329a47d2bd38317fce26fc85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce8256917bda46b998d9cc1d12d0ab9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "94c5a05d22db4d489fff96cda7400f9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc9c3f1ab61b44a0af226c56d2d23fb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d17135271bce4cd0ac04cf9d99df4f0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4eea957cdf7c4112b7ad573dc3a0c4d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simulate111/Basics-of-Programming---Exercise/blob/main/Exercise%20task%205%3A%20Feature%20importance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "\n",
        "Before we start running our own Python code, install the required Python packages using [pip](https://en.wikipedia.org/wiki/Pip):\n",
        "\n",
        "* [`transformers`](https://huggingface.co/docs/transformers/index) is a popular deep learning package primarily on top of torch, we need to reinstall it with the [torch] configuration (might take a substantial amount of time)\n",
        "* [`datasets`](https://huggingface.co/docs/datasets/) provides support for loading, creating, and manipulating datasets\n",
        "* evaluate is a library of performance metrics (like accuracy etc)\n",
        "\n",
        "**You will likely need to do a Runtime/Restart session for everything to work after the installation.**"
      ],
      "metadata": {
        "id": "LDZZUKzfPRWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install -q datasets evaluate\n",
        "!pip install transformers[torch]"
      ],
      "metadata": {
        "id": "fKPHBYptQDsK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7535f879-e47b-412c-950f-576915be1c08"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.13.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.2.1+cu121)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.28.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch]) (12.4.99)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Above, the `!` at the start of the line tells the notebook to run the line as an operating system command rather than Python code, and the `-q` argument to `pip` runs the command in \"quiet\" mode, with less output.)"
      ],
      "metadata": {
        "id": "xOeI-LA9RcYL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Get and prepare data\n",
        "\n",
        "*   Let us work with the IMDB dataset of movie review sentiment\n",
        "*   25,000 positive reviews\n",
        "*   25,000 negative reviews\n",
        "*   50,000 unlabeled reviews (which we discard for the time being)\n"
      ],
      "metadata": {
        "id": "_fCdfQfNNzwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint #pprint => pretty-print, I use it occassionally throughout the notebook\n",
        "import datasets\n",
        "import torch\n",
        "dset=datasets.load_dataset(\"imdb\")\n",
        "pprint(dset)"
      ],
      "metadata": {
        "id": "QxmgHoKDTN2_",
        "outputId": "d50600f7-5bc7-4d10-c5ea-4447e71ffb6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 25000\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 25000\n",
            "    })\n",
            "    unsupervised: Dataset({\n",
            "        features: ['text', 'label'],\n",
            "        num_rows: 50000\n",
            "    })\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dset=dset.shuffle() #This is never a bad idea, datasets may have ordering to them, which is not what we want\n",
        "del dset[\"unsupervised\"] #Delete the unlabeled part of the dataset, we don't need it for anything"
      ],
      "metadata": {
        "id": "lbmgiOj4pWw-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(dset['train'][0]['text'])\n",
        "print(dset['train'][0]['label'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KI3kyD3vry8C",
        "outputId": "e8b8f66c-9926-40bd-e12f-d1a9fd930e10"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(\"According to IMDb Takashi Miike's Master of Horror-segment, Imprint, was \"\n",
            " \"banned in the US. So I figured I'd translate the Swedish review I just wrote \"\n",
            " 'for it...<br /><br />It was hard to NOT have any sort of expectations from '\n",
            " \"Ichi The Killer-director Takashi Miike's episode in the Masters of Horror \"\n",
            " 'series. And the DVD-cover of Imprint did in deed look very promising.<br '\n",
            " '/><br />The story mostly takes place in a remote Japanese bordello, some '\n",
            " 'time during the 19th century, and it tells the tale of a journalist '\n",
            " 'searching for Komomo, the woman he left behind and whom he promised to '\n",
            " 'return for. Tired and dejected he arrives at the bordello, hoping that this '\n",
            " 'will be the end of his very long journey. It turns out that one of the '\n",
            " 'prostitutes, a deformed and quiet girl, know about Komomo, and the desperate '\n",
            " 'man makes her tell him where she is and what has happened to her since he '\n",
            " 'left. The story she tells him is as deplorable as it is hard to '\n",
            " 'swallow...<br /><br />The first thing that hit me about the episode was how '\n",
            " 'unnatural it seemed that the Japanese cast for the most part spoke fluent '\n",
            " \"American-English. But I will leave it at that, it's not that big a deal. \"\n",
            " \"What IS a big deal however is how miserable the rest of it was. Miike's tale \"\n",
            " \"moves at such a slow pace that I couldn't help looking at my watch several \"\n",
            " 'times during the 63 minutes. The extended torure-scene, that takes place '\n",
            " 'somewhere in the middle of the movie, felt so unmotivated - and '\n",
            " 'pornographically intrusive - that not even THAT scene became interesting. I '\n",
            " 'felt like it was violent just for the sake of violence itself - with no '\n",
            " 'sense of style or purpose. The only scenes that provoked any kind of emotion '\n",
            " 'out of me were the images of bloody fetuses rolling along the bottom of the '\n",
            " 'swiftly flowing water...and, in all honesty, the only emotions they provoked '\n",
            " 'were feelings of disgust.<br /><br />The journalist seeking the love he left '\n",
            " 'behind is played by Billy Drago, for me most memorable as Frank Nitti - Al '\n",
            " \"Capones whiteclad assassin in Brian De Palmas The Untouchables (1987). I've \"\n",
            " 'always found Dragos portrayal of Nitti to be very icy (and I mean that in a '\n",
            " 'good way), and that is probably why I was almost annoyed when I found him to '\n",
            " 'be so terrible (NOT in a good way) in this one. His acting seems to flow '\n",
            " 'between no feelings or empathy whatsoever to displays of some really bad '\n",
            " 'overacting. When his character is supposed to react to the awful things '\n",
            " 'Komomo has been subjected to I was sitting in the sofa, twisting and turning '\n",
            " \"in an attempt to escape the horrible actingjob put forth by Drago. I'm \"\n",
            " 'grateful that most of the story is told by Yuoki Kudoh (Memoirs of a geisha, '\n",
            " '2005), who plays the deformed prostitute.<br /><br />The finale is probably '\n",
            " 'supposed to be chocking, maybe even revolting and horrid, but I just found '\n",
            " 'it to be kind of...you know... \"blah\" (and I looked at my watch again, for '\n",
            " 'the umptieth time, just wishing the crappy episode would end). Maybe the '\n",
            " \"finale caused me to smile just a bit, but that's only because I couldn't \"\n",
            " 'help thinking of an episode of Red Dwarf, and the upside-down chins of Craig '\n",
            " 'Charles and Danny John-Jules, with eyes glued on them to make them look like '\n",
            " \"aliens... Lucky you, if you've seen that episode and now decide to see \"\n",
            " 'Imprint, I will forever have ruined the visuals of the ending for you.<br '\n",
            " '/><br />My first thought, when Imprint finally ended, was that the only '\n",
            " 'thing that made the pain of watching it worth it, was hearing the main title '\n",
            " 'theme by Edward Shearmur (the same music I believe is used in every episode '\n",
            " \"of this series), and that - if anything - is a big friggin warning, don't \"\n",
            " 'you think?<br /><br />One might point to the costume design, by Michiko '\n",
            " 'Kitamura, and say that there, at least, is something NOT lacking in style '\n",
            " 'and refinement...but there are so many other films and TV-shows that is so '\n",
            " 'much better at showing off the Japanese \"geisha-fashion\". This is nothing '\n",
            " \"but inferior and I am disappointed. Takashi Miike's Masters of \"\n",
            " \"Horror-episode is boring, uninspiring and pointless. In other words; It's \"\n",
            " 'really, really BAD!')\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenize and map vocabulary\n",
        "         \n",
        "*   We need to achieve two complementary tasks\n",
        "*   **Tokenize** split the text into units which can be interpreted as features (words in this case)\n",
        "*   **Map vocabulary** build the feature vector for each example\n",
        "*   Since this is NLP, here it means listing the non-zero elements of the feature vector, or in other words the indices of the vocabulary items\n",
        "* Since we work with the bag of words (BoW) representation, these do not need to be (and are not) in the order in which they appear in the text\n",
        "* These indices then refer to the rows in the embedding matrix\n",
        "*   A traditional and well-tested way it to use sklearn's feature extraction package\n",
        "*   CountVectorizer is most likely what we want in here, because we only want the ids, nothing else\n",
        "* But for other NLP work the TfidfVectorizer is also very handy\n",
        "\n"
      ],
      "metadata": {
        "id": "KMnqQ78cMpk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.feature_extraction\n",
        "\n",
        "# max_features means the size of the vocabulary\n",
        "# which means max_features most-common words\n",
        "vectorizer=sklearn.feature_extraction.text.CountVectorizer(binary=True,max_features=20000)\n",
        "\n",
        "texts=[ex[\"text\"] for ex in dset[\"train\"]] #get a list of all texts from the training data\n",
        "vectorizer.fit(texts) #\"Trains\" the vectorizer, i.e. builds its vocabulary\n"
      ],
      "metadata": {
        "id": "MYOtKJu7Mohd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "f79c3aa5-5188-4037-bbed-00acffdfad87"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer(binary=True, max_features=20000)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(binary=True, max_features=20000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(binary=True, max_features=20000)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building the feature vectors\n",
        "\n",
        "* This is super-easy with the vectorizer\n",
        "* It produces a sparse matrix of the non-zero elements"
      ],
      "metadata": {
        "id": "UDNy3w8reO6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_example(ex):\n",
        "    vectorized=vectorizer.transform([ex[\"text\"]]) # [...] because the vectorizer expects a list/iterable over inputs, not one input\n",
        "    non_zero_features=vectorized.nonzero()[1] #.nonzero gives a pair of (rows,columns), we want the columns\n",
        "    non_zero_features+=1 #feature index 0 will have a special meaning\n",
        "                         # so let us not produce it by adding +1 to everything\n",
        "    return {\"input_ids\":non_zero_features}\n",
        "\n",
        "vectorized=vectorize_example(dset[\"train\"][0])"
      ],
      "metadata": {
        "id": "5H9AQOc2rGIO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vectorized)"
      ],
      "metadata": {
        "id": "ECqsgmwTXaOC",
        "outputId": "b92e1334-fe79-4aa8-fb36-dfe28feab868",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': array([  121,   135,   144,   215,   309,   373,   419,   605,   676,\n",
            "         720,   727,   761,   764,   793,   794,   826,   860,   887,\n",
            "         951,  1000,  1005,  1115,  1173,  1207,  1240,  1300,  1330,\n",
            "        1463,  1517,  1594,  1702,  1736,  1737,  1756,  1791,  1807,\n",
            "        1897,  1901,  1925,  1951,  1980,  2012,  2094,  2231,  2257,\n",
            "        2295,  2377,  2604,  2625,  2878,  2926,  2991,  3053,  3072,\n",
            "        4121,  4131,  4172,  4214,  4225,  4565,  4629,  4635,  4687,\n",
            "        4719,  4761,  4896,  4950,  4964,  5089,  5167,  5185,  5241,\n",
            "        5283,  5429,  5490,  5697,  5711,  5714,  5828,  5990,  5994,\n",
            "        5999,  6044,  6051,  6052,  6087,  6180,  6235,  6302,  6315,\n",
            "        6449,  6524,  6560,  6688,  6768,  6782,  6866,  6887,  6895,\n",
            "        6896,  6939,  7056,  7060,  7065,  7127,  7161,  7204,  7222,\n",
            "        7259,  7332,  7346,  7557,  7682,  7756,  7801,  7914,  8225,\n",
            "        8241,  8289,  8322,  8350,  8380,  8454,  8476,  8567,  8592,\n",
            "        8602,  8680,  8718,  8733,  8735,  8740,  8780,  8783,  8889,\n",
            "        8895,  8929,  8968,  8983,  9085,  9226,  9428,  9512,  9602,\n",
            "        9630,  9639,  9691,  9782,  9824,  9827,  9862,  9890, 10016,\n",
            "       10029, 10056, 10090, 10160, 10328, 10330, 10341, 10475, 10628,\n",
            "       10638, 10640, 10642, 10696, 10736, 10829, 10871, 10886, 10890,\n",
            "       10915, 10970, 11076, 11083, 11137, 11176, 11181, 11260, 11261,\n",
            "       11364, 11376, 11381, 11460, 11487, 11721, 11723, 11761, 11762,\n",
            "       11778, 11832, 11857, 12134, 12202, 12212, 12237, 12363, 12364,\n",
            "       12437, 12439, 12445, 12504, 12564, 12577, 12630, 12707, 12733,\n",
            "       12858, 13285, 13325, 13333, 13396, 13400, 13497, 13778, 13859,\n",
            "       13861, 13910, 13911, 13947, 14048, 14065, 14120, 14316, 14349,\n",
            "       14443, 14486, 14645, 14825, 14877, 14906, 14921, 15138, 15253,\n",
            "       15347, 15370, 15461, 15502, 15505, 15642, 15682, 15689, 15692,\n",
            "       15695, 15696, 15700, 15746, 15790, 15830, 15919, 16045, 16049,\n",
            "       16161, 16204, 16359, 16400, 16478, 16506, 16542, 16550, 16555,\n",
            "       16592, 16767, 17075, 17223, 17240, 17302, 17412, 17495, 17521,\n",
            "       17536, 17628, 17633, 17635, 17781, 17784, 17843, 17893, 17897,\n",
            "       17910, 17913, 17929, 17944, 17951, 17952, 17954, 17955, 17968,\n",
            "       17983, 18074, 18081, 18095, 18107, 18115, 18135, 18314, 18525,\n",
            "       18526, 18531, 18548, 18784, 18833, 18835, 18943, 18959, 18963,\n",
            "       19051, 19112, 19195, 19196, 19234, 19382, 19398, 19414, 19420,\n",
            "       19421, 19440, 19521, 19540, 19544, 19549, 19551, 19590, 19599,\n",
            "       19609, 19639, 19705, 19712, 19741, 19770, 19801, 19805, 19853,\n",
            "       19927], dtype=int32)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can map back to vocabulary and check that everything works\n",
        "# vectorizer.vocabulary_ is a dictionary {key:word, value:idx}\n",
        "\n",
        "idx2word=dict((i,w) for (w,i) in vectorizer.vocabulary_.items()) #inverse the vocab dictionary\n",
        "words=[]\n",
        "for idx in vectorized[\"input_ids\"]:\n",
        "    words.append(idx2word[idx-1]) ## It is easy to forgot we moved all by +1\n",
        "pprint(\", \".join(words)) #This is now the bag of words representation of the document"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvgd3IrWfCai",
        "outputId": "a89a4fa8-e848-4465-be95-9fabfa012cfc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('1987, 19th, 2005, 63, about, according, acting, again, al, aliens, all, '\n",
            " 'almost, along, always, am, american, an, and, annoyed, any, anything, are, '\n",
            " 'arrives, as, assassin, at, attempt, awful, bad, banned, be, became, because, '\n",
            " 'been, behind, believe, better, between, big, billy, bit, blah, bloody, '\n",
            " 'boring, bottom, br, brian, but, by, cast, caused, century, character, '\n",
            " 'charles, costume, couldn, cover, craig, crappy, danny, de, deal, decide, '\n",
            " 'deed, deformed, deplorable, design, desperate, did, director, disappointed, '\n",
            " 'disgust, displays, don, down, during, dvd, dwarf, edward, emotion, emotions, '\n",
            " 'empathy, end, ended, ending, english, episode, escape, even, every, '\n",
            " 'expectations, extended, eyes, fashion, feelings, felt, figured, films, '\n",
            " 'finale, finally, first, flow, flowing, fluent, for, forever, forth, found, '\n",
            " 'frank, friggin, from, geisha, girl, glued, good, grateful, happened, hard, '\n",
            " 'has, have, he, hearing, help, her, him, his, hit, honesty, hoping, horrible, '\n",
            " 'horrid, horror, how, however, ichi, icy, if, images, imdb, in, inferior, '\n",
            " 'interesting, intrusive, is, it, itself, japanese, john, journalist, journey, '\n",
            " 'jules, just, killer, kind, kitamura, know, lacking, least, leave, left, '\n",
            " 'like, long, look, looked, looking, love, lucky, made, main, make, makes, '\n",
            " 'man, many, master, masters, maybe, me, mean, memoirs, memorable, middle, '\n",
            " 'might, miike, minutes, miserable, most, mostly, moves, movie, much, music, '\n",
            " 'my, no, not, nothing, now, of, off, on, one, only, or, other, out, '\n",
            " 'overacting, pace, pain, part, place, played, plays, point, pointless, '\n",
            " 'portrayal, probably, promised, promising, prostitute, prostitutes, provoked, '\n",
            " 'purpose, put, quiet, react, really, red, refinement, remote, rest, return, '\n",
            " 'review, revolting, rolling, ruined, sake, same, say, scene, scenes, '\n",
            " 'searching, see, seeking, seemed, seems, seen, segment, sense, series, '\n",
            " 'several, she, showing, shows, since, sitting, slow, smile, so, sofa, some, '\n",
            " 'something, somewhere, sort, spoke, story, style, subjected, such, supposed, '\n",
            " 'swallow, swedish, swiftly, takashi, takes, tale, tell, tells, terrible, '\n",
            " 'that, the, them, theme, there, they, thing, things, think, thinking, this, '\n",
            " 'thought, time, times, tired, title, to, told, translate, turning, turns, tv, '\n",
            " 'twisting, uninspiring, unmotivated, unnatural, upside, us, used, ve, very, '\n",
            " 'violence, violent, visuals, warning, was, watch, watching, water, way, were, '\n",
            " 'what, whatsoever, when, where, who, whom, why, will, wishing, with, woman, '\n",
            " 'words, worth, would, wrote, you')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizing / vectorizing the whole dataset\n",
        "\n",
        "* The datasets library allows us to efficiently map() a function across the whole dataset\n",
        "* Can run in parallel\n",
        "\n",
        "**Note**: confusingly, and unlike the Python`map` function, [`Dataset.map`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map) function _updates_ its argument dataset, keeping existing values. Here, the call adds the values returned by the function call (here `input_ids`) to each example while also keeping the original `text` and `label` values.\n"
      ],
      "metadata": {
        "id": "i33KIMgkiIn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the tokenizer to the whole dataset using .map()\n",
        "dset_tokenized = dset.map(vectorize_example,num_proc=4)\n",
        "pprint(dset_tokenized[\"train\"][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "7ad86de09ed04a039eca6365e64e466e",
            "92e4dc7b53794da39021519b75773186",
            "b2ce206f0d9746e491de89fd732f1f75",
            "7d028b8fff074f2aa1b4846b8cf3449d",
            "10e6cf26c3744d44900173a92228aaf4",
            "2f7c0905db02458ba16eb5b24fd9e3db",
            "9ea0d7f22301421fb9245708a7e5d49b",
            "ab2ff9d720a040b58e5a9136695fa20f",
            "3ce8b6ec4d894d03845a0aad3ff12d0b",
            "1fe50785bccf46c0ba56b068d1e271b1",
            "e25bada82ee84d3db5a58ca7bd2a9e71",
            "002744b49222446fbf8026508a6564c4",
            "2eb66e05993e4adca1ac779611d36e8c",
            "006d453afb4d4573a0b9dad2491789e8",
            "91c7bbeb27d544559d4e340bce695ab7",
            "a341e94e390a4cf28df754922b48fe5f",
            "34e993cb329a47d2bd38317fce26fc85",
            "ce8256917bda46b998d9cc1d12d0ab9e",
            "94c5a05d22db4d489fff96cda7400f9d",
            "fc9c3f1ab61b44a0af226c56d2d23fb0",
            "d17135271bce4cd0ac04cf9d99df4f0d",
            "4eea957cdf7c4112b7ad573dc3a0c4d4"
          ]
        },
        "id": "33xMYRd0q2B9",
        "outputId": "632e7c55-4130-4d49-c073-2f6e7408fd16"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map (num_proc=4):   0%|          | 0/25000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7ad86de09ed04a039eca6365e64e466e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map (num_proc=4):   0%|          | 0/25000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "002744b49222446fbf8026508a6564c4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [121,\n",
            "               135,\n",
            "               144,\n",
            "               215,\n",
            "               309,\n",
            "               373,\n",
            "               419,\n",
            "               605,\n",
            "               676,\n",
            "               720,\n",
            "               727,\n",
            "               761,\n",
            "               764,\n",
            "               793,\n",
            "               794,\n",
            "               826,\n",
            "               860,\n",
            "               887,\n",
            "               951,\n",
            "               1000,\n",
            "               1005,\n",
            "               1115,\n",
            "               1173,\n",
            "               1207,\n",
            "               1240,\n",
            "               1300,\n",
            "               1330,\n",
            "               1463,\n",
            "               1517,\n",
            "               1594,\n",
            "               1702,\n",
            "               1736,\n",
            "               1737,\n",
            "               1756,\n",
            "               1791,\n",
            "               1807,\n",
            "               1897,\n",
            "               1901,\n",
            "               1925,\n",
            "               1951,\n",
            "               1980,\n",
            "               2012,\n",
            "               2094,\n",
            "               2231,\n",
            "               2257,\n",
            "               2295,\n",
            "               2377,\n",
            "               2604,\n",
            "               2625,\n",
            "               2878,\n",
            "               2926,\n",
            "               2991,\n",
            "               3053,\n",
            "               3072,\n",
            "               4121,\n",
            "               4131,\n",
            "               4172,\n",
            "               4214,\n",
            "               4225,\n",
            "               4565,\n",
            "               4629,\n",
            "               4635,\n",
            "               4687,\n",
            "               4719,\n",
            "               4761,\n",
            "               4896,\n",
            "               4950,\n",
            "               4964,\n",
            "               5089,\n",
            "               5167,\n",
            "               5185,\n",
            "               5241,\n",
            "               5283,\n",
            "               5429,\n",
            "               5490,\n",
            "               5697,\n",
            "               5711,\n",
            "               5714,\n",
            "               5828,\n",
            "               5990,\n",
            "               5994,\n",
            "               5999,\n",
            "               6044,\n",
            "               6051,\n",
            "               6052,\n",
            "               6087,\n",
            "               6180,\n",
            "               6235,\n",
            "               6302,\n",
            "               6315,\n",
            "               6449,\n",
            "               6524,\n",
            "               6560,\n",
            "               6688,\n",
            "               6768,\n",
            "               6782,\n",
            "               6866,\n",
            "               6887,\n",
            "               6895,\n",
            "               6896,\n",
            "               6939,\n",
            "               7056,\n",
            "               7060,\n",
            "               7065,\n",
            "               7127,\n",
            "               7161,\n",
            "               7204,\n",
            "               7222,\n",
            "               7259,\n",
            "               7332,\n",
            "               7346,\n",
            "               7557,\n",
            "               7682,\n",
            "               7756,\n",
            "               7801,\n",
            "               7914,\n",
            "               8225,\n",
            "               8241,\n",
            "               8289,\n",
            "               8322,\n",
            "               8350,\n",
            "               8380,\n",
            "               8454,\n",
            "               8476,\n",
            "               8567,\n",
            "               8592,\n",
            "               8602,\n",
            "               8680,\n",
            "               8718,\n",
            "               8733,\n",
            "               8735,\n",
            "               8740,\n",
            "               8780,\n",
            "               8783,\n",
            "               8889,\n",
            "               8895,\n",
            "               8929,\n",
            "               8968,\n",
            "               8983,\n",
            "               9085,\n",
            "               9226,\n",
            "               9428,\n",
            "               9512,\n",
            "               9602,\n",
            "               9630,\n",
            "               9639,\n",
            "               9691,\n",
            "               9782,\n",
            "               9824,\n",
            "               9827,\n",
            "               9862,\n",
            "               9890,\n",
            "               10016,\n",
            "               10029,\n",
            "               10056,\n",
            "               10090,\n",
            "               10160,\n",
            "               10328,\n",
            "               10330,\n",
            "               10341,\n",
            "               10475,\n",
            "               10628,\n",
            "               10638,\n",
            "               10640,\n",
            "               10642,\n",
            "               10696,\n",
            "               10736,\n",
            "               10829,\n",
            "               10871,\n",
            "               10886,\n",
            "               10890,\n",
            "               10915,\n",
            "               10970,\n",
            "               11076,\n",
            "               11083,\n",
            "               11137,\n",
            "               11176,\n",
            "               11181,\n",
            "               11260,\n",
            "               11261,\n",
            "               11364,\n",
            "               11376,\n",
            "               11381,\n",
            "               11460,\n",
            "               11487,\n",
            "               11721,\n",
            "               11723,\n",
            "               11761,\n",
            "               11762,\n",
            "               11778,\n",
            "               11832,\n",
            "               11857,\n",
            "               12134,\n",
            "               12202,\n",
            "               12212,\n",
            "               12237,\n",
            "               12363,\n",
            "               12364,\n",
            "               12437,\n",
            "               12439,\n",
            "               12445,\n",
            "               12504,\n",
            "               12564,\n",
            "               12577,\n",
            "               12630,\n",
            "               12707,\n",
            "               12733,\n",
            "               12858,\n",
            "               13285,\n",
            "               13325,\n",
            "               13333,\n",
            "               13396,\n",
            "               13400,\n",
            "               13497,\n",
            "               13778,\n",
            "               13859,\n",
            "               13861,\n",
            "               13910,\n",
            "               13911,\n",
            "               13947,\n",
            "               14048,\n",
            "               14065,\n",
            "               14120,\n",
            "               14316,\n",
            "               14349,\n",
            "               14443,\n",
            "               14486,\n",
            "               14645,\n",
            "               14825,\n",
            "               14877,\n",
            "               14906,\n",
            "               14921,\n",
            "               15138,\n",
            "               15253,\n",
            "               15347,\n",
            "               15370,\n",
            "               15461,\n",
            "               15502,\n",
            "               15505,\n",
            "               15642,\n",
            "               15682,\n",
            "               15689,\n",
            "               15692,\n",
            "               15695,\n",
            "               15696,\n",
            "               15700,\n",
            "               15746,\n",
            "               15790,\n",
            "               15830,\n",
            "               15919,\n",
            "               16045,\n",
            "               16049,\n",
            "               16161,\n",
            "               16204,\n",
            "               16359,\n",
            "               16400,\n",
            "               16478,\n",
            "               16506,\n",
            "               16542,\n",
            "               16550,\n",
            "               16555,\n",
            "               16592,\n",
            "               16767,\n",
            "               17075,\n",
            "               17223,\n",
            "               17240,\n",
            "               17302,\n",
            "               17412,\n",
            "               17495,\n",
            "               17521,\n",
            "               17536,\n",
            "               17628,\n",
            "               17633,\n",
            "               17635,\n",
            "               17781,\n",
            "               17784,\n",
            "               17843,\n",
            "               17893,\n",
            "               17897,\n",
            "               17910,\n",
            "               17913,\n",
            "               17929,\n",
            "               17944,\n",
            "               17951,\n",
            "               17952,\n",
            "               17954,\n",
            "               17955,\n",
            "               17968,\n",
            "               17983,\n",
            "               18074,\n",
            "               18081,\n",
            "               18095,\n",
            "               18107,\n",
            "               18115,\n",
            "               18135,\n",
            "               18314,\n",
            "               18525,\n",
            "               18526,\n",
            "               18531,\n",
            "               18548,\n",
            "               18784,\n",
            "               18833,\n",
            "               18835,\n",
            "               18943,\n",
            "               18959,\n",
            "               18963,\n",
            "               19051,\n",
            "               19112,\n",
            "               19195,\n",
            "               19196,\n",
            "               19234,\n",
            "               19382,\n",
            "               19398,\n",
            "               19414,\n",
            "               19420,\n",
            "               19421,\n",
            "               19440,\n",
            "               19521,\n",
            "               19540,\n",
            "               19544,\n",
            "               19549,\n",
            "               19551,\n",
            "               19590,\n",
            "               19599,\n",
            "               19609,\n",
            "               19639,\n",
            "               19705,\n",
            "               19712,\n",
            "               19741,\n",
            "               19770,\n",
            "               19801,\n",
            "               19805,\n",
            "               19853,\n",
            "               19927],\n",
            " 'label': 0,\n",
            " 'text': \"According to IMDb Takashi Miike's Master of Horror-segment, Imprint, \"\n",
            "         \"was banned in the US. So I figured I'd translate the Swedish review \"\n",
            "         'I just wrote for it...<br /><br />It was hard to NOT have any sort '\n",
            "         \"of expectations from Ichi The Killer-director Takashi Miike's \"\n",
            "         'episode in the Masters of Horror series. And the DVD-cover of '\n",
            "         'Imprint did in deed look very promising.<br /><br />The story mostly '\n",
            "         'takes place in a remote Japanese bordello, some time during the 19th '\n",
            "         'century, and it tells the tale of a journalist searching for Komomo, '\n",
            "         'the woman he left behind and whom he promised to return for. Tired '\n",
            "         'and dejected he arrives at the bordello, hoping that this will be '\n",
            "         'the end of his very long journey. It turns out that one of the '\n",
            "         'prostitutes, a deformed and quiet girl, know about Komomo, and the '\n",
            "         'desperate man makes her tell him where she is and what has happened '\n",
            "         'to her since he left. The story she tells him is as deplorable as it '\n",
            "         'is hard to swallow...<br /><br />The first thing that hit me about '\n",
            "         'the episode was how unnatural it seemed that the Japanese cast for '\n",
            "         'the most part spoke fluent American-English. But I will leave it at '\n",
            "         \"that, it's not that big a deal. What IS a big deal however is how \"\n",
            "         \"miserable the rest of it was. Miike's tale moves at such a slow pace \"\n",
            "         \"that I couldn't help looking at my watch several times during the 63 \"\n",
            "         'minutes. The extended torure-scene, that takes place somewhere in '\n",
            "         'the middle of the movie, felt so unmotivated - and pornographically '\n",
            "         'intrusive - that not even THAT scene became interesting. I felt like '\n",
            "         'it was violent just for the sake of violence itself - with no sense '\n",
            "         'of style or purpose. The only scenes that provoked any kind of '\n",
            "         'emotion out of me were the images of bloody fetuses rolling along '\n",
            "         'the bottom of the swiftly flowing water...and, in all honesty, the '\n",
            "         'only emotions they provoked were feelings of disgust.<br /><br />The '\n",
            "         'journalist seeking the love he left behind is played by Billy Drago, '\n",
            "         'for me most memorable as Frank Nitti - Al Capones whiteclad assassin '\n",
            "         \"in Brian De Palmas The Untouchables (1987). I've always found Dragos \"\n",
            "         'portrayal of Nitti to be very icy (and I mean that in a good way), '\n",
            "         'and that is probably why I was almost annoyed when I found him to be '\n",
            "         'so terrible (NOT in a good way) in this one. His acting seems to '\n",
            "         'flow between no feelings or empathy whatsoever to displays of some '\n",
            "         'really bad overacting. When his character is supposed to react to '\n",
            "         'the awful things Komomo has been subjected to I was sitting in the '\n",
            "         'sofa, twisting and turning in an attempt to escape the horrible '\n",
            "         \"actingjob put forth by Drago. I'm grateful that most of the story is \"\n",
            "         'told by Yuoki Kudoh (Memoirs of a geisha, 2005), who plays the '\n",
            "         'deformed prostitute.<br /><br />The finale is probably supposed to '\n",
            "         'be chocking, maybe even revolting and horrid, but I just found it to '\n",
            "         'be kind of...you know... \"blah\" (and I looked at my watch again, for '\n",
            "         'the umptieth time, just wishing the crappy episode would end). Maybe '\n",
            "         \"the finale caused me to smile just a bit, but that's only because I \"\n",
            "         \"couldn't help thinking of an episode of Red Dwarf, and the \"\n",
            "         'upside-down chins of Craig Charles and Danny John-Jules, with eyes '\n",
            "         \"glued on them to make them look like aliens... Lucky you, if you've \"\n",
            "         'seen that episode and now decide to see Imprint, I will forever have '\n",
            "         'ruined the visuals of the ending for you.<br /><br />My first '\n",
            "         'thought, when Imprint finally ended, was that the only thing that '\n",
            "         'made the pain of watching it worth it, was hearing the main title '\n",
            "         'theme by Edward Shearmur (the same music I believe is used in every '\n",
            "         'episode of this series), and that - if anything - is a big friggin '\n",
            "         \"warning, don't you think?<br /><br />One might point to the costume \"\n",
            "         'design, by Michiko Kitamura, and say that there, at least, is '\n",
            "         'something NOT lacking in style and refinement...but there are so '\n",
            "         'many other films and TV-shows that is so much better at showing off '\n",
            "         'the Japanese \"geisha-fashion\". This is nothing but inferior and I am '\n",
            "         \"disappointed. Takashi Miike's Masters of Horror-episode is boring, \"\n",
            "         \"uninspiring and pointless. In other words; It's really, really BAD!\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Input encoding for MLP\n",
        "\n",
        "* Our `input_ids` are an array containing the indices of the tokens found in the text\n",
        "* This corresponds to the indices into the row of the embedding matrix in the model\n",
        "* That seems to be exactly what we need!\n"
      ],
      "metadata": {
        "id": "XTtyHQpIIJWw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batching and padding\n",
        "\n",
        "* When working with neural networks, one rarely trains one example at a time\n",
        "* Instead, processing always happens a batch at a time\n",
        "* This has two important reasons:\n",
        "  1. No batching is too slow (GPU parallelization cannot kick in across examples)\n",
        "  2. The gradients are averaged across the whole batch and applied only once, i.e. batching acts as a regularizer and improves the stability of the training\n"
      ],
      "metadata": {
        "id": "JvaP1DpHjI3f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Padding and Collation (forming a batch)\n",
        "\n",
        "## Padding:\n",
        "\n",
        "* In order to build a batch as a 2D array of (example, seq), we need to fit together examples of different length\n",
        "* Solution: pad the shorter examples with zeroes to the length of the longest example in the batch\n",
        "* Make sure that zero is understood as padding value rather than a (hypothetical) feature with index 0\n",
        "* This is best shown by example, it is in the end easier than it may sound\n",
        "\n",
        "## Collation:\n",
        "\n",
        "* Much like examples are dictionaries with the data, also batches are dictionaries with the data\n",
        "* The only difference is that in a batch, all data tensors have one extra dimension, that's all there is to it\n",
        "\n",
        "## Collator function:\n",
        "\n",
        "* Padding and collation is taken care of by a single function in the HF libraries\n",
        "* It receives a list of examples, and returns a ready batch\n",
        "* The surrounding library code takes care of forming these lists\n",
        "* Let's try to implement one below"
      ],
      "metadata": {
        "id": "hNYxw92nj51B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) I need to define it here, will explain below\n",
        "# 2) I show here a very straightforward implementation of padding and collation\n",
        "# 3) Normally, one would use transformers.DataCollatorWithPadding but that assumes\n",
        "#    a particular tokenizer, to which it outsources much of the work, and we do not\n",
        "#    have it\n",
        "def collator(list_of_examples):\n",
        "    #this is easy, labels are made into a single tensor\n",
        "    batch={\"labels\":torch.tensor(list(ex[\"label\"] for ex in list_of_examples))}\n",
        "    #the worse bit is now to pad the examples, as they are of different length\n",
        "    tensors=[]\n",
        "    max_len=max(len(example[\"input_ids\"]) for example in list_of_examples) #this is the longest example in the batch\n",
        "    #everything needs to be padded to fit in length the longest example\n",
        "    #(so we can build a single tensor out of it)\n",
        "    for example in list_of_examples:\n",
        "        ids=torch.tensor(example[\"input_ids\"]) #pick the input ids\n",
        "        # pad(what,(from_left, from_right)) <- this is how we call the stock pad function\n",
        "        padded=torch.nn.functional.pad(ids,(0,max_len-ids.shape[0])) #pad by max - current length, pads with zero by default\n",
        "        tensors.append(padded) #accumulated the padded ids\n",
        "    batch[\"input_ids\"]=torch.vstack(tensors) #now that we have all of them the same length, a simple vstack() stacks them up\n",
        "    return batch #...and that's all there is to it\n",
        "\n",
        "\n",
        "\n",
        "#Build a batch from 2 examples, with padding\n",
        "batch=collator([dset_tokenized[\"train\"][2],dset_tokenized[\"train\"][7]])\n",
        "print(\"Shape of labels:\",batch[\"labels\"].shape)\n",
        "print(\"Shape of input_ids:\",batch[\"input_ids\"].shape)\n",
        "pprint(batch[\"labels\"])\n",
        "pprint(batch[\"input_ids\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXXDxNo6kwfA",
        "outputId": "fb05db89-ba57-459e-ef75-9355bf0f3f07"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of labels: torch.Size([2])\n",
            "Shape of input_ids: torch.Size([2, 100])\n",
            "tensor([1, 0])\n",
            "tensor([[  309,   419,   468,   594,   774,   887,   979,  1115,  1207,  1300,\n",
            "          1778,  2016,  2084,  2199,  2625,  2762,  3181,  3259,  3516,  3606,\n",
            "          3891,  3964,  4077,  4476,  4606,  4974,  5434,  5451,  5697,  6343,\n",
            "          6367,  6368,  6640,  6672,  6705,  7102,  7127,  7222,  7327,  7346,\n",
            "          7536,  7694,  8592,  8780,  9085,  9602,  9630,  9890, 10017, 10322,\n",
            "         10829, 11176, 11321, 11619, 11694, 11746, 11762, 11857, 12202, 12363,\n",
            "         12439, 12451, 12632, 12792, 13087, 13497, 13757, 14327, 14349, 14619,\n",
            "         14985, 15074, 15085, 15267, 15330, 15502, 15583, 16381, 16407, 16608,\n",
            "         17018, 17776, 17820, 17893, 17897, 17907, 17917, 17968, 18115, 18353,\n",
            "         18469, 18553, 19273, 19398, 19420, 19509, 19562, 19712, 19773, 19847],\n",
            "        [  137,   211,   224,   594,   727,   749,   887,  1207,  1300,  1573,\n",
            "          1594,  1667,  1793,  2344,  2604,  2709,  2878,  2997,  3984,  4971,\n",
            "          5167,  5646,  5746,  5771,  6311,  7127,  7346,  7476,  8322,  8649,\n",
            "          8942,  9085,  9630, 10090, 10246, 10398, 10449, 10475, 10952, 11135,\n",
            "         11180, 11256, 11261, 11267, 11321, 11721, 11762, 11771, 12084, 12202,\n",
            "         12363, 12410, 12439, 12521, 13326, 13801, 13805, 14131, 14349, 15682,\n",
            "         15695, 15830, 16001, 16017, 16048, 16105, 16161, 16849, 17893, 17897,\n",
            "         17954, 17968, 18074, 18115, 18808, 19398, 19446, 19521, 19540, 19551,\n",
            "         19562, 19712, 19785, 19805, 19897, 19928,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build the MLP model\n",
        "\n",
        "* Now that all of our data is in shape, we can build the model\n",
        "* That is luckily quite easy in this case\n",
        "\n",
        "The model class in its simplest form has `__init__()` which instantiates the layers and `forward()` which implements the actual computation. For more information on these, please see the [PyTorch turorial](https://pytorch.org/tutorials/beginner/introyt/modelsyt_tutorial.html)."
      ],
      "metadata": {
        "id": "AOYYF5I1OWG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "\n",
        "# A model wants a config, I can simply inherit from the base\n",
        "# class for pretrained configs\n",
        "class MLPConfig(transformers.PretrainedConfig):\n",
        "    pass\n",
        "\n",
        "# This is the model\n",
        "class MLP(transformers.PreTrainedModel):\n",
        "\n",
        "    config_class=MLPConfig\n",
        "\n",
        "    # In the initialization method, one instantiates the layers\n",
        "    # these will be, for the most part the trained parameters of the model\n",
        "    def __init__(self,config):\n",
        "        super().__init__(config)\n",
        "        self.vocab_size=config.vocab_size #embedding matrix row count\n",
        "\n",
        "        ##############################################################################################################\n",
        "        #modify the size of the embedding matrix (learns a single real number for every feature)\n",
        "        # Build and initialize embedding of vocab size +1 x hidden size (+1 because of the padding index 0!)\n",
        "        #self.embedding=torch.nn.Embedding(num_embeddings=self.vocab_size+1,embedding_dim=config.hidden_size,padding_idx=0)\n",
        "        self.embedding=torch.nn.Embedding(num_embeddings=self.vocab_size+1,embedding_dim=1,padding_idx=0)\n",
        "        ####################################################################################################################\n",
        "\n",
        "        # Normally you would not initialize these yourself, but I have my reasons here ;)\n",
        "        torch.nn.init.uniform_(self.embedding.weight.data,-0.001,0.001) #initialize the embeddings with small random values\n",
        "        # Note! This function is relatively clever and keeps the embedding for 0, the padding, pure zeros\n",
        "        # This takes care of the lower half of the network, now the upper half\n",
        "\n",
        "        ################################################################################################################\n",
        "        #OUtput layer has the size of 1 now.\n",
        "        # Output layer: hidden size x output size\n",
        "        #self.output=torch.nn.Linear(in_features=config.hidden_size,out_features=config.nlabels)\n",
        "        self.output = torch.nn.Linear(in_features=1, out_features=config.nlabels)\n",
        "        ##############################################################################################################\n",
        "        # Now we have the parameters of the model\n",
        "\n",
        "\n",
        "    # The computation of the model is put into the forward() function\n",
        "    # it receives a batch of data and optionally the correct `labels`\n",
        "    #\n",
        "    # If given `labels` it returns (loss,output)\n",
        "    # if not, then it returns (output,)\n",
        "    def forward(self,input_ids,labels=None):\n",
        "        #1) sum up the embeddings of the items\n",
        "        embedded=self.embedding(input_ids) #(batch,ids)->(batch,ids,embedding_dim)\n",
        "        # Since the Embedding keeps the first row of the matrix pure zeros, we don't need to worry about the padding\n",
        "        # so next we sum the embeddings across the word dimension\n",
        "        # (batch,ids,embedding_dim) -> (batch,embedding_dim)\n",
        "        embedded_summed=torch.sum(embedded,dim=1)\n",
        "\n",
        "        #2) apply non-linearity\n",
        "        # (batch,embedding_dim) -> (batch,embedding_dim)\n",
        "\n",
        "        #tanh funtion has non-linearity which should be removed to achieve linear classifier.\n",
        "        ###############################################################################################################################\n",
        "        ##projected=torch.tanh(embedded_summed) #Note how non-linearity is applied here and not when configuring the layer in __init__()\n",
        "        projected =embedded_summed\n",
        "        ###############################################################################################################################\n",
        "\n",
        "\n",
        "        #3) and now apply the upper, output layer of the network\n",
        "        # (batch,embedding_dim) -> (batch, num_of_classes i.e. 2 in our case)\n",
        "        logits=self.output(projected)\n",
        "\n",
        "        # ...and that's all there is to it!\n",
        "\n",
        "        #print(\"input_ids.shape\",input_ids.shape)\n",
        "        #print(\"embedded.shape\",embedded.shape)\n",
        "        #print(\"embedded_summed.shape\",embedded_summed.shape)\n",
        "        #print(\"projected.shape\",projected.shape)\n",
        "        #print(\"logits.shape\",logits.shape)\n",
        "\n",
        "        # If we have labels, we ought to calculate the loss\n",
        "        if labels is not None:\n",
        "            loss=torch.nn.CrossEntropyLoss() #This loss is meant for classification, so let's use it\n",
        "            # You run it as loss(model_output,correct_labels)\n",
        "            return (loss(logits,labels),logits)\n",
        "        else:\n",
        "            # No labels, so just return the logits\n",
        "            return (logits,)\n",
        "\n"
      ],
      "metadata": {
        "id": "vP4FtrMwCpGi"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure the model:\n",
        "#   these parameters are used in the model's __init__()\n",
        "mlp_config=MLPConfig(vocab_size=len(vectorizer.vocabulary_),hidden_size=20,nlabels=2)\n",
        "\n",
        "# And now we can instantiate it\n",
        "mlp=MLP(mlp_config)\n",
        "\n",
        "#we can make a little test with a fake batch formed by the two first example\n",
        "fake_batch=collator([dset_tokenized[\"train\"][0],dset_tokenized[\"train\"][1]])\n",
        "mlp(**fake_batch) #** expands input_ids and labels as parameters of the call"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQjSGUYGwrzh",
        "outputId": "ad5e578c-e4fe-49ee-add7-1e62f2292c6b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.8799, grad_fn=<NllLossBackward0>),\n",
              " tensor([[-0.5395,  0.7211],\n",
              "         [-0.3927,  0.8680]], grad_fn=<AddmmBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the model\n",
        "\n",
        "We will use the Hugging Face [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer) class for training\n",
        "\n",
        "* Loads of arguments that control the training\n",
        "* Configurable metrics to evaluate performance\n",
        "* Data collator builds the batches\n",
        "* Early stopping callback stops when eval loss no longer improves\n",
        "* Model load/save\n",
        "* Excellent foundation for later deep learning course\n",
        "  "
      ],
      "metadata": {
        "id": "tdlcMObzQGGP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's create a [`TrainingArguments`](https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/trainer#transformers.TrainingArguments) object to specify hyperparameters and various other settings for training.\n",
        "\n",
        "Printing this simple dataclass object will show not only the values we set, but also the defaults for all other arguments. Don't worry if you don't understand what all of these do! Many are not relevant to us here, and you can find the details in [`Trainer` documentation](https://huggingface.co/docs/transformers/main_classes/trainer) if you are interested."
      ],
      "metadata": {
        "id": "aZgcNi4B76SX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set training arguments\n",
        "# their names are mostly self-explanatory\n",
        "trainer_args = transformers.TrainingArguments(\n",
        "    \"mlp_checkpoints\", #save checkpoints here\n",
        "    evaluation_strategy=\"steps\",\n",
        "    logging_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    logging_steps=500,\n",
        "    learning_rate=1e-5, #learning rate of the gradient descent\n",
        "    max_steps=20000,\n",
        "    load_best_model_at_end=True,\n",
        "    per_device_train_batch_size=128\n",
        ")\n",
        "\n",
        "pprint(trainer_args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhfdW62z8cCn",
        "outputId": "169de1fc-02d6-45f7-8d9d-e0cc9a097038"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TrainingArguments(\n",
            "_n_gpu=0,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=500,\n",
            "evaluation_strategy=steps,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=False,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=1e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=True,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=mlp_checkpoints/runs/Mar31_14-30-27_387de529acf7,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=20000,\n",
            "metric_for_best_model=loss,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=mlp_checkpoints,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=128,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=mlp_checkpoints,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's create a metric for evaluating performance during and after training. We can use the convenience function [`load_metric`](https://huggingface.co/docs/datasets/about_metrics) to load one of many pre-made metrics and wrap this for use by the trainer.\n",
        "\n",
        "As the task is simple binary classification and our data is even 50:50 balanced, we can comfortably use the basic `accuracy` metric, defined as the proportion of correctly predicted labels out of all labels."
      ],
      "metadata": {
        "id": "4sJwNXPU-dQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_accuracy(outputs_and_labels):\n",
        "    outputs, labels = outputs_and_labels\n",
        "    predictions = np.argmax(outputs, axis=-1) #pick the index of the \"winning\" label\n",
        "    return accuracy.compute(predictions=predictions, references=labels)"
      ],
      "metadata": {
        "id": "u3jxIItb0BL9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can then create the `Trainer` and train the model by invoking the [`Trainer.train`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Trainer.train) function.\n",
        "\n",
        "In addition to the model, the settings passed in through the `TrainingArguments` object created above (`trainer_args`), the data, and the metric defined above, we create and pass the following to the `Trainer`:\n",
        "\n",
        "* [data collator](https://huggingface.co/docs/transformers/main_classes/data_collator): groups input into batches\n",
        "* [`EarlyStoppingCallback`](https://huggingface.co/docs/transformers/main_classes/callback#transformers.EarlyStoppingCallback): stops training when performance stops improving"
      ],
      "metadata": {
        "id": "S7kbz8uU-zpQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a new model\n",
        "mlp = MLP(mlp_config)\n",
        "\n",
        "\n",
        "# Argument gives the number of steps of patience before early stopping\n",
        "# i.e. training is stopped when the evaluation loss fails to improve\n",
        "# certain number of times\n",
        "early_stopping = transformers.EarlyStoppingCallback(5)\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=mlp,\n",
        "    args=trainer_args,\n",
        "    train_dataset=dset_tokenized[\"train\"],\n",
        "    eval_dataset=dset_tokenized[\"test\"].select(range(1000)), #make a smaller subset to evaluate on\n",
        "    compute_metrics=compute_accuracy,\n",
        "    data_collator=collator,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# FINALLY!\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "AoEoWsj4P_zN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2179e621-81f5-4e4f-bf32-16a41b6c0ecb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='20000' max='20000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [20000/20000 08:54, Epoch 102/103]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.800400</td>\n",
              "      <td>0.809762</td>\n",
              "      <td>0.481000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.768200</td>\n",
              "      <td>0.778947</td>\n",
              "      <td>0.481000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.742700</td>\n",
              "      <td>0.755150</td>\n",
              "      <td>0.480000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.721900</td>\n",
              "      <td>0.737478</td>\n",
              "      <td>0.481000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.708100</td>\n",
              "      <td>0.723980</td>\n",
              "      <td>0.496000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.697300</td>\n",
              "      <td>0.713430</td>\n",
              "      <td>0.505000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.689000</td>\n",
              "      <td>0.704653</td>\n",
              "      <td>0.513000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.681100</td>\n",
              "      <td>0.697070</td>\n",
              "      <td>0.529000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.674600</td>\n",
              "      <td>0.690129</td>\n",
              "      <td>0.536000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.668300</td>\n",
              "      <td>0.683646</td>\n",
              "      <td>0.544000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.661900</td>\n",
              "      <td>0.677494</td>\n",
              "      <td>0.553000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.656400</td>\n",
              "      <td>0.671641</td>\n",
              "      <td>0.560000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.650200</td>\n",
              "      <td>0.665963</td>\n",
              "      <td>0.563000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.644900</td>\n",
              "      <td>0.660527</td>\n",
              "      <td>0.570000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>0.639900</td>\n",
              "      <td>0.655319</td>\n",
              "      <td>0.581000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.634200</td>\n",
              "      <td>0.650247</td>\n",
              "      <td>0.589000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>0.629500</td>\n",
              "      <td>0.645421</td>\n",
              "      <td>0.598000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>0.624200</td>\n",
              "      <td>0.640761</td>\n",
              "      <td>0.605000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>0.619900</td>\n",
              "      <td>0.636356</td>\n",
              "      <td>0.613000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>0.615200</td>\n",
              "      <td>0.632081</td>\n",
              "      <td>0.616000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10500</td>\n",
              "      <td>0.611300</td>\n",
              "      <td>0.628044</td>\n",
              "      <td>0.625000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11000</td>\n",
              "      <td>0.606300</td>\n",
              "      <td>0.624190</td>\n",
              "      <td>0.632000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11500</td>\n",
              "      <td>0.603400</td>\n",
              "      <td>0.620635</td>\n",
              "      <td>0.639000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12000</td>\n",
              "      <td>0.599100</td>\n",
              "      <td>0.617223</td>\n",
              "      <td>0.646000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12500</td>\n",
              "      <td>0.596200</td>\n",
              "      <td>0.614041</td>\n",
              "      <td>0.656000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13000</td>\n",
              "      <td>0.593100</td>\n",
              "      <td>0.611042</td>\n",
              "      <td>0.665000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13500</td>\n",
              "      <td>0.589800</td>\n",
              "      <td>0.608301</td>\n",
              "      <td>0.667000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14000</td>\n",
              "      <td>0.586800</td>\n",
              "      <td>0.605755</td>\n",
              "      <td>0.676000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14500</td>\n",
              "      <td>0.585000</td>\n",
              "      <td>0.603423</td>\n",
              "      <td>0.679000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15000</td>\n",
              "      <td>0.582400</td>\n",
              "      <td>0.601296</td>\n",
              "      <td>0.680000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15500</td>\n",
              "      <td>0.580000</td>\n",
              "      <td>0.599356</td>\n",
              "      <td>0.685000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16000</td>\n",
              "      <td>0.578400</td>\n",
              "      <td>0.597626</td>\n",
              "      <td>0.690000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16500</td>\n",
              "      <td>0.576300</td>\n",
              "      <td>0.596135</td>\n",
              "      <td>0.691000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17000</td>\n",
              "      <td>0.575500</td>\n",
              "      <td>0.594827</td>\n",
              "      <td>0.692000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17500</td>\n",
              "      <td>0.574000</td>\n",
              "      <td>0.593726</td>\n",
              "      <td>0.695000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18000</td>\n",
              "      <td>0.572400</td>\n",
              "      <td>0.592813</td>\n",
              "      <td>0.696000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18500</td>\n",
              "      <td>0.572200</td>\n",
              "      <td>0.592113</td>\n",
              "      <td>0.698000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19000</td>\n",
              "      <td>0.571500</td>\n",
              "      <td>0.591609</td>\n",
              "      <td>0.699000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19500</td>\n",
              "      <td>0.571000</td>\n",
              "      <td>0.591309</td>\n",
              "      <td>0.699000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20000</td>\n",
              "      <td>0.571000</td>\n",
              "      <td>0.591207</td>\n",
              "      <td>0.699000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Checkpoint destination directory mlp_checkpoints/checkpoint-500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory mlp_checkpoints/checkpoint-1000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory mlp_checkpoints/checkpoint-1500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory mlp_checkpoints/checkpoint-2000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory mlp_checkpoints/checkpoint-2500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory mlp_checkpoints/checkpoint-3000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory mlp_checkpoints/checkpoint-3500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory mlp_checkpoints/checkpoint-4000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory mlp_checkpoints/checkpoint-4500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory mlp_checkpoints/checkpoint-5000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory mlp_checkpoints/checkpoint-5500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory mlp_checkpoints/checkpoint-6000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory mlp_checkpoints/checkpoint-6500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory mlp_checkpoints/checkpoint-7000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory mlp_checkpoints/checkpoint-7500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory mlp_checkpoints/checkpoint-8000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory mlp_checkpoints/checkpoint-8500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory mlp_checkpoints/checkpoint-9000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory mlp_checkpoints/checkpoint-9500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory mlp_checkpoints/checkpoint-10000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory mlp_checkpoints/checkpoint-10500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory mlp_checkpoints/checkpoint-11000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory mlp_checkpoints/checkpoint-11500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory mlp_checkpoints/checkpoint-12000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory mlp_checkpoints/checkpoint-12500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory mlp_checkpoints/checkpoint-13000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory mlp_checkpoints/checkpoint-13500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory mlp_checkpoints/checkpoint-14000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory mlp_checkpoints/checkpoint-14500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory mlp_checkpoints/checkpoint-15000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory mlp_checkpoints/checkpoint-15500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory mlp_checkpoints/checkpoint-16000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory mlp_checkpoints/checkpoint-16500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory mlp_checkpoints/checkpoint-17000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory mlp_checkpoints/checkpoint-17500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory mlp_checkpoints/checkpoint-18000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory mlp_checkpoints/checkpoint-18500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory mlp_checkpoints/checkpoint-19000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory mlp_checkpoints/checkpoint-19500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n",
            "Checkpoint destination directory mlp_checkpoints/checkpoint-20000 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=20000, training_loss=0.6305891738891601, metrics={'train_runtime': 534.7838, 'train_samples_per_second': 4786.981, 'train_steps_per_second': 37.398, 'total_flos': 26167556544.0, 'train_loss': 0.6305891738891601, 'epoch': 102.04})"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can then evaluate the trained model on a given dataset (here our test subset) by calling [`Trainer.evaluate`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Trainer.evaluate):"
      ],
      "metadata": {
        "id": "Td03fcIa-6Hj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval_results = trainer.evaluate(dset_tokenized[\"test\"])\n",
        "\n",
        "print(eval_results)"
      ],
      "metadata": {
        "id": "9nlEwpnF2Vow",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "74db47ef-c59a-41a8-b5d9-400f721a2abc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3125/3125 00:06]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.5900015234947205, 'eval_accuracy': 0.71268, 'eval_runtime': 6.9512, 'eval_samples_per_second': 3596.494, 'eval_steps_per_second': 449.562, 'epoch': 102.04}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save the model for later use\n",
        "\n",
        "* You can save it with `trainer.save_model()`\n",
        "* You can load it with `MLP.from_pretrained()`\n"
      ],
      "metadata": {
        "id": "P0OYB3TRp-IH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"mlp-imdb\")"
      ],
      "metadata": {
        "id": "FosHTDw3p9dd"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check save/load"
      ],
      "metadata": {
        "id": "8D8W9EN-zMAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mlp2=MLP.from_pretrained(\"mlp-imdb\")"
      ],
      "metadata": {
        "id": "UKMMNbABqLTg"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = transformers.Trainer(\n",
        "    model=mlp2,\n",
        "    args=trainer_args,\n",
        "    train_dataset=dset_tokenized[\"train\"],\n",
        "    eval_dataset=dset_tokenized[\"test\"],\n",
        "    compute_metrics=compute_accuracy,\n",
        "    data_collator=collator,\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ],
      "metadata": {
        "id": "pfLnUENrrCyp",
        "outputId": "b3cd3267-5d93-4413-8e89-393121738cb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_results = trainer.evaluate(dset_tokenized[\"test\"])\n",
        "print(eval_results)\n",
        "print('Accuracy:', eval_results['eval_accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "hpOM2ZwErMGf",
        "outputId": "1f3c6eb4-be76-4be3-ae83-fae44ff28d9f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3125/3125 00:08]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.5900015234947205, 'eval_accuracy': 0.71268, 'eval_runtime': 8.5711, 'eval_samples_per_second': 2916.767, 'eval_steps_per_second': 364.596}\n",
            "Accuracy: 0.71268\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extra time left?\n",
        "\n",
        "* Read through the TrainingArguments documentation, try to understand at least some parts of it https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments\n",
        "* Read through Torch tensor operations, try to understand at least some parts of it: https://pytorch.org/docs/stable/tensors.html\n",
        "* Run the model with different parameters (hidden layer width, learning rate, etc), how much do the results change?\n"
      ],
      "metadata": {
        "id": "8UT5MV1LtSBh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What has the model learned?\n",
        "\n",
        "* The embeddings should have some meaning to them\n",
        "* Similar features should have similar embeddings"
      ],
      "metadata": {
        "id": "13aB7DuqzeFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grab the embedding matrix out of the trained model\n",
        "# and drop the first row (padding 0)\n",
        "# then we can treat the embeddings as vectors\n",
        "# and maybe compare them to each other\n",
        "# ha ha this below took some googling\n",
        "weights=mlp.embedding.weight.detach().cpu().numpy()\n",
        "weights=weights[1:,:]"
      ],
      "metadata": {
        "id": "M6TUrVkMCmz7"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qry_idx=vectorizer.vocabulary_[\"great\"] #embedding of \"great\"\n",
        "\n",
        "#calculate the distance of the \"lousy\" embedding to all other embeddings\n",
        "distance_to_qry=sklearn.metrics.pairwise.euclidean_distances(weights[qry_idx:qry_idx+1,:],weights)\n",
        "nearest_neighbors=np.argsort(distance_to_qry) #indices of words nearest to \"lousy\"\n",
        "for nearest in nearest_neighbors[0,:20]:\n",
        "    print(idx2word[nearest])\n",
        "# This works great!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uTTvRgGDU9D",
        "outputId": "94b46d00-d95a-4a56-d8cb-b293a6580278"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "great\n",
            "excellent\n",
            "wonderful\n",
            "best\n",
            "perfect\n",
            "amazing\n",
            "loved\n",
            "favorite\n",
            "beautiful\n",
            "love\n",
            "superb\n",
            "highly\n",
            "brilliant\n",
            "fantastic\n",
            "today\n",
            "enjoyed\n",
            "beautifully\n",
            "wonderfully\n",
            "touching\n",
            "very\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The embeddings indeed seem to reflect the task\n",
        "* There is a meaning to them"
      ],
      "metadata": {
        "id": "cQCmThdu2LDR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature weights\n",
        "\n",
        "*   A typical \"old-school\" way to approach the classification would be a simple linear model, like LinearSVM\n",
        "*   Under such model, each feature (word) would have a single one weight\n",
        "*   And the classification would simply be based on the sum of these weights\n",
        "*   In this context of this task, \"positive\" words would get a high weight, \"negative\" words would get a low weight\n",
        "*   It is in fact quite easy to reconfigure the MLP model to work more or less like this and this effect can be replicated\n",
        "*   I will leave that as an exercise for you\n",
        "\n"
      ],
      "metadata": {
        "id": "Jydy3ECK3O2w"
      }
    }
  ]
}